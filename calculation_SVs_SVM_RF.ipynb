{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Calculation of SVERAD SVs (for SVM) and SHAP values (for RF and SVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7f63d26",
   "metadata": {},
   "source": [
    "This notebook will generate:\n",
    "\n",
    "* Exact SVs for SVM with SVERAD\n",
    "* SHAP values for SVM with KernelSHAP\n",
    "* (Exact?) SVs for RF with TreeExplainer\n",
    "* SHAP values for RF with KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.sverad_svm import ExplainingSVC, create_SVC\n",
    "from src.sverad_kernel import rbf_kernel_matrix_sparse\n",
    "from src.utils import DataSet, UnfoldedMorganFingerprint, set_seeds\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shap\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm.auto import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57533678",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "\n",
    "SAVE_DATASET_PICKLE = False\n",
    "LOAD_DATASET_PICKLE = True\n",
    "LOAD_PRECOMPUTED_EXPLANATIONS = False\n",
    "# SAVE_EXPLANATIONS = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Definition of models and searched hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f99c68d-da61-4f51-b115-9d46e6802def",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_list = [{\"name\": \"SVC\",\n",
    "               \"algorithm\": ExplainingSVC(),\n",
    "               \"parameter\": {\"C\": [0.1, 1, 10, 50, 100, 200, 400, 500, 750, 1000], \n",
    "                            \"gamma_value\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], #[0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "                            },\n",
    "              },\n",
    "              {\"name\": \"RF\",\n",
    "               \"algorithm\": ensemble.RandomForestClassifier(random_state=SEED, bootstrap=True, max_features=\"sqrt\"),\n",
    "               \"parameter\": {'n_estimators':[10, 100, 250, 500],\n",
    "                             'min_samples_split': [2, 3, 5, 7, 10,],\n",
    "                             'min_samples_leaf':[1, 2, 5, 10],\n",
    "                            },\n",
    "              },\n",
    "             ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the pre-compiled compound set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa0f5a4-922b-48cf-b206-b83c8d7a7160",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>active</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniprot_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P0DMS8</th>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label       active  random\n",
       "uniprot_id                \n",
       "P0DMS8         287     287"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(\"./data/dataset.tsv\", sep=\"\\t\")\n",
    "dataset_df.pivot_table(index=\"uniprot_id\", columns=\"label\", values=\"nonstereo_aromatic_smiles\", aggfunc=\"nunique\", fill_value=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c3f3f77-4538-49b1-8277-fada73a415da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dict = dict()\n",
    "fingerprint_gen_dict = dict()\n",
    "\n",
    "if not LOAD_DATASET_PICKLE:\n",
    "    for dataset_name, data_grpd_df in dataset_df.groupby(\"uniprot_id\"):\n",
    "        \n",
    "        # label: 1: active, 0: random\n",
    "        labels = np.array([1 if l == 'active' else 0 for l in data_grpd_df.label])\n",
    "        # Creating Fingerprint\n",
    "        morgan_radius2 = UnfoldedMorganFingerprint(radius=2)\n",
    "        morgan_radius2.fit_smiles(data_grpd_df.nonstereo_aromatic_smiles.tolist())\n",
    "        \n",
    "        # Constructing Dataset\n",
    "        fp_matrix = morgan_radius2.transform_smiles(data_grpd_df.nonstereo_aromatic_smiles.tolist())\n",
    "        # Constructing Dataset\n",
    "        dataset = DataSet(labels, fp_matrix)\n",
    "        dataset.add_attribute(\"nonstereo_aromatic_smiles\", data_grpd_df.nonstereo_aromatic_smiles.values)\n",
    "        \n",
    "        dataset_dict[dataset_name] = dataset\n",
    "        fingerprint_gen_dict[dataset_name] = morgan_radius2\n",
    "else:\n",
    "    with open(\"./data/pickle/dataset_dict.p\", \"rb\") as infile:\n",
    "        dataset_dict = pickle.load(infile)\n",
    "    with open(\"./data/pickle/fingerprint_gen_dict.p\", \"rb\") as infile:\n",
    "        fingerprint_gen_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06da5da4-6871-4638-aba6-f03faabaa9d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_DATASET_PICKLE:\n",
    "    with open(\"./pickle_dumps/dataset_dict.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(dataset_dict, outfile)\n",
    "    with open(\"./pickle_dumps/fingerprint_gen_dict.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(fingerprint_gen_dict, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the models and generation of feature contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93538af2-4829-476f-b61e-c0fe0e10068f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_splits = 1 # Number of test-training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e071b433-9e2d-4348-93d9-c5142d25203b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81aa190ff004352bc35d4f296e84eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset.\n",
      "{'C': [0.1, 1, 10, 50, 100, 200, 400, 500, 750, 1000], 'gamma_value': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
      "Model fitting and tuning to obtain optimal hyperparameters via Grid Search...\n",
      "Model accuracy:  0.9337979094076655\n",
      "Model trained and optimized.\n",
      "Model name:  SVC\n",
      "Explaining model using SVERAD...\n",
      "SVERAD values computation done.\n",
      "Explaining model using SHAP KernelExplainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7338a90bb57641f5b94a21fea8f55562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Repositories/SV-RBF/calculation_SVs_SVM_RF.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Repositories/SV-RBF/calculation_SVs_SVM_RF.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Repositories/SV-RBF/calculation_SVs_SVM_RF.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExplaining model using SHAP KernelExplainer...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Repositories/SV-RBF/calculation_SVs_SVM_RF.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     kernel_shap_values \u001b[39m=\u001b[39m model_explainer\u001b[39m.\u001b[39;49mshap_values(dataset\u001b[39m.\u001b[39;49mfeature_matrix, nsamples\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Repositories/SV-RBF/calculation_SVs_SVM_RF.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSHAP KernelExplainer done.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Repositories/SV-RBF/calculation_SVs_SVM_RF.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39m# Creating a DataFrame with all relevant data.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sveta/lib/python3.10/site-packages/shap/explainers/_kernel.py:190\u001b[0m, in \u001b[0;36mKernel.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_index:\n\u001b[1;32m    189\u001b[0m     data \u001b[39m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m], index_name)\n\u001b[0;32m--> 190\u001b[0m explanations\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain(data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mgc_collect\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    192\u001b[0m     gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/anaconda3/envs/sveta/lib/python3.10/site-packages/shap/explainers/_kernel.py:388\u001b[0m, in \u001b[0;36mKernel.explain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m phi_var \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mgroups_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD))\n\u001b[1;32m    387\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD):\n\u001b[0;32m--> 388\u001b[0m     vphi, vphi_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolve(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnsamples \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_samples, d)\n\u001b[1;32m    389\u001b[0m     phi[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvaryingInds, d] \u001b[39m=\u001b[39m vphi\n\u001b[1;32m    390\u001b[0m     phi_var[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvaryingInds, d] \u001b[39m=\u001b[39m vphi_var\n",
      "File \u001b[0;32m~/anaconda3/envs/sveta/lib/python3.10/site-packages/shap/explainers/_kernel.py:565\u001b[0m, in \u001b[0;36mKernel.solve\u001b[0;34m(self, fraction_evaluated, dim)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1_reg \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1_reg \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbic\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1_reg \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39maic\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    564\u001b[0m     c \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39maic\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1_reg \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1_reg\n\u001b[0;32m--> 565\u001b[0m     nonzero_inds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnonzero(LassoLarsIC(criterion\u001b[39m=\u001b[39;49mc)\u001b[39m.\u001b[39;49mfit(mask_aug, eyAdj_aug)\u001b[39m.\u001b[39mcoef_)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    567\u001b[0m \u001b[39m# use a fixed regularization coeffcient\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     nonzero_inds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnonzero(Lasso(alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1_reg)\u001b[39m.\u001b[39mfit(mask_aug, eyAdj_aug)\u001b[39m.\u001b[39mcoef_)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/sveta/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:2195\u001b[0m, in \u001b[0;36mLassoLarsIC.fit\u001b[0;34m(self, X, y, copy_X)\u001b[0m\n\u001b[1;32m   2189\u001b[0m X, y, Xmean, ymean, Xstd \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[1;32m   2190\u001b[0m     X, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept, _normalize, copy_X\n\u001b[1;32m   2191\u001b[0m )\n\u001b[1;32m   2193\u001b[0m Gram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecompute\n\u001b[0;32m-> 2195\u001b[0m alphas_, _, coef_path_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m lars_path(\n\u001b[1;32m   2196\u001b[0m     X,\n\u001b[1;32m   2197\u001b[0m     y,\n\u001b[1;32m   2198\u001b[0m     Gram\u001b[39m=\u001b[39;49mGram,\n\u001b[1;32m   2199\u001b[0m     copy_X\u001b[39m=\u001b[39;49mcopy_X,\n\u001b[1;32m   2200\u001b[0m     copy_Gram\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2201\u001b[0m     alpha_min\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m   2202\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlasso\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2203\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   2204\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   2205\u001b[0m     eps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m   2206\u001b[0m     return_n_iter\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2207\u001b[0m     positive\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpositive,\n\u001b[1;32m   2208\u001b[0m )\n\u001b[1;32m   2210\u001b[0m n_samples \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39maic\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sveta/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:169\u001b[0m, in \u001b[0;36mlars_path\u001b[0;34m(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m Gram \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mX cannot be None if Gram is not None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse lars_path_gram to avoid passing X and y.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[39mreturn\u001b[39;00m _lars_path_solver(\n\u001b[1;32m    170\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    171\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    172\u001b[0m     Xy\u001b[39m=\u001b[39;49mXy,\n\u001b[1;32m    173\u001b[0m     Gram\u001b[39m=\u001b[39;49mGram,\n\u001b[1;32m    174\u001b[0m     n_samples\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     max_iter\u001b[39m=\u001b[39;49mmax_iter,\n\u001b[1;32m    176\u001b[0m     alpha_min\u001b[39m=\u001b[39;49malpha_min,\n\u001b[1;32m    177\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    178\u001b[0m     copy_X\u001b[39m=\u001b[39;49mcopy_X,\n\u001b[1;32m    179\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m     copy_Gram\u001b[39m=\u001b[39;49mcopy_Gram,\n\u001b[1;32m    181\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    182\u001b[0m     return_path\u001b[39m=\u001b[39;49mreturn_path,\n\u001b[1;32m    183\u001b[0m     return_n_iter\u001b[39m=\u001b[39;49mreturn_n_iter,\n\u001b[1;32m    184\u001b[0m     positive\u001b[39m=\u001b[39;49mpositive,\n\u001b[1;32m    185\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/sveta/lib/python3.10/site-packages/sklearn/linear_model/_least_angle.py:726\u001b[0m, in \u001b[0;36m_lars_path_solver\u001b[0;34m(X, y, Xy, Gram, n_samples, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\u001b[0m\n\u001b[1;32m    721\u001b[0m     corr_eq_dir \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(X\u001b[39m.\u001b[39mT[n_active:], eq_dir)\n\u001b[1;32m    722\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    723\u001b[0m     \u001b[39m# if huge number of features, this takes 50% of time, I\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[39m# think could be avoided if we just update it using an\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     \u001b[39m# orthogonal (QR) decomposition of X\u001b[39;00m\n\u001b[0;32m--> 726\u001b[0m     corr_eq_dir \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(Gram[:n_active, n_active:]\u001b[39m.\u001b[39;49mT, least_squares)\n\u001b[1;32m    728\u001b[0m \u001b[39m# Explicit rounding can be necessary to avoid `np.argmax(Cov)` yielding\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[39m# unstable results because of rounding errors.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m np\u001b[39m.\u001b[39maround(corr_eq_dir, decimals\u001b[39m=\u001b[39mcov_precision, out\u001b[39m=\u001b[39mcorr_eq_dir)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prediction_df = []\n",
    "failed = 0\n",
    "hyperparamter_dict = dict()\n",
    "obtained_models = dict()\n",
    "shap_dict = dict()\n",
    "\n",
    "# Loop over multiple data-sets. Here only one is assessed.\n",
    "for dataset_name, dataset in tqdm(dataset_dict.items(), total=len(dataset_dict)): #it takes ~5:30h to run Kernel SHAP for SVC + ~7h for RF\n",
    "\n",
    "    # Loop over test-training splits. Only one is assessed. `n_splits == 1`\n",
    "    print(\"Splitting dataset.\")\n",
    "    data_splitter = StratifiedShuffleSplit(n_splits=n_splits, random_state=SEED, test_size=0.50)\n",
    "    for trial_nr, (train_idx, test_idx) in tqdm(enumerate(data_splitter.split(dataset.feature_matrix, dataset.label)), leave=False, total=n_splits, disable=True):\n",
    "        training_set = dataset[train_idx]\n",
    "        test_set = dataset[test_idx]\n",
    "\n",
    "        #Iterating over assessed models.\n",
    "        for model_dict in model_list:\n",
    "            print(model_dict[\"parameter\"])\n",
    "            # Setting up hyperparameter search.\n",
    "            param_grid = model_dict[\"parameter\"]\n",
    "            model = GridSearchCV(estimator = model_dict[\"algorithm\"],\n",
    "                                 param_grid = param_grid,\n",
    "                                 n_jobs=1,\n",
    "                                 scoring= \"neg_mean_squared_error\", #\"neg_mean_squared_error\", \"accuracy\"\n",
    "                                 cv=StratifiedShuffleSplit(n_splits=10, random_state=SEED, test_size=0.5),\n",
    "                                verbose=0,\n",
    "                                )\n",
    "            # Determining optimal hyperparameters and fitting the model to the entire training set with these hyperparamters\n",
    "            print(\"Model fitting and tuning to obtain optimal hyperparameters via Grid Search...\")\n",
    "            model.fit(training_set.feature_matrix, training_set.label)\n",
    "            preds = model.predict(test_set.feature_matrix)\n",
    "            print(\"Model accuracy: \", np.mean(preds == test_set.label))\n",
    "            obtained_models[(dataset_name, trial_nr, model_dict[\"name\"])] = model\n",
    "            print(\"Model trained and optimized.\")\n",
    "            # Saving hyperparameters\n",
    "            if model_dict[\"name\"] not in hyperparamter_dict:\n",
    "                hyperparamter_dict[model_dict[\"name\"]] = []\n",
    "            best_param = dict(model.best_params_)\n",
    "            best_param[\"dataset_name\"] = dataset_name\n",
    "            best_param[\"trial\"] = trial_nr\n",
    "            hyperparamter_dict[model_dict[\"name\"]].append(best_param)\n",
    "\n",
    "            # break\n",
    "            # SVs.\n",
    "            shap_values = None\n",
    "            print(\"Model name: \", model_dict[\"name\"])\n",
    "            if model_dict[\"name\"] == \"RF\":  # Random forest\n",
    "                pass #skip RF explanations since we already have those\n",
    "                # model_explainer = shap.TreeExplainer(model.best_estimator_, feature_perturbation=\"interventional\", data=training_set.feature_matrix.toarray())\n",
    "                # try:\n",
    "                #     # first line to sort out cases which fail on the spot\n",
    "                #     _ = model_explainer.shap_values(test_set.feature_matrix.toarray()[0:2, :], check_additivity=True)  # check_additivity is True in defalult. This is just a reminder why I'm doing this.\n",
    "                #     print(\"Explaining model using SHAP TreeExplainer...\")\n",
    "                #     shap_values = model_explainer.shap_values(dataset.feature_matrix.toarray())[1]\n",
    "                #     expected_value = model_explainer.expected_value[1]\n",
    "                #     print(\"SHAP TreeExplainer done.\")\n",
    "                # except Exception as ex:\n",
    "                #     print(ex)\n",
    "                #     failed += 1\n",
    "                #     continue\n",
    "            elif model_dict[\"name\"] == \"SVC\":\n",
    "                print(\"Explaining model using SVERAD...\")\n",
    "                shap_values = model.best_estimator_.feature_weights(dataset.feature_matrix)\n",
    "                expected_value = model.best_estimator_.expected_value\n",
    "                print(\"SVERAD values computation done.\")\n",
    "            else:\n",
    "                raise(ValueError(\"Model not implemented.\"))\n",
    "            \n",
    "            # Kernel SHAP\n",
    "            shap_sample = shap.sample(training_set.feature_matrix)\n",
    "            if model_dict[\"name\"] == \"RF\":\n",
    "                link = \"identity\"\n",
    "            else:\n",
    "                link = \"logit\"\n",
    "            model_explainer = shap.KernelExplainer(model.predict_proba, shap_sample, link=link)\n",
    "            with warnings.catch_warnings():\n",
    "                # ignore all caught warnings. Necessary, since LassoLarsIC raises (a lot) future-warnings . See https://github.com/slundberg/shap/issues/2528\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                print(\"Explaining model using SHAP KernelExplainer...\")\n",
    "                kernel_shap_values = model_explainer.shap_values(dataset.feature_matrix, nsamples=\"auto\")[1]\n",
    "                print(\"SHAP KernelExplainer done.\")\n",
    "            # Creating a DataFrame with all relevant data.\n",
    "            trial_df = pd.DataFrame()\n",
    "            trial_df[\"nonstereo_aromatic_smiles\"] = dataset.nonstereo_aromatic_smiles\n",
    "            trial_df[\"dataset_idx\"] = range(len(dataset.label))\n",
    "            trial_df[\"label\"] = dataset.label\n",
    "            trial_df[\"prediction\"] = model.best_estimator_.predict(dataset.feature_matrix)\n",
    "            if model_dict[\"name\"] == \"SVC\":\n",
    "                trial_df[\"log_odds\"] = model.best_estimator_.predict_log_odds(dataset.feature_matrix)[:, 1]\n",
    "            trial_df[\"proba\"] = model.best_estimator_.predict_proba(dataset.feature_matrix)[:, 1]\n",
    "            trial_df[\"trainingset\"] = trial_df.nonstereo_aromatic_smiles.isin(training_set.nonstereo_aromatic_smiles)\n",
    "            trial_df[\"testset\"] = trial_df.nonstereo_aromatic_smiles.isin(test_set.nonstereo_aromatic_smiles)\n",
    "            trial_df[\"trial\"] = trial_nr\n",
    "            trial_df[\"dataset_name\"] = dataset_name\n",
    "            trial_df[\"algorithm\"] = model_dict[\"name\"]\n",
    "            \n",
    "            # Creating a data set storing all SVs and SHAP values.\n",
    "            shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])] = dict()\n",
    "            if shap_values is not None:\n",
    "                trial_df[\"present_shap\"] = (shap_values * dataset.feature_matrix.toarray()).sum(axis=1)\n",
    "                trial_df[\"absent_shap\"] = (shap_values * (1-dataset.feature_matrix.toarray())).sum(axis=1) \n",
    "                if model_dict[\"name\"] == \"SVC\":\n",
    "                    shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"sverad_values\"] = shap_values\n",
    "                else:\n",
    "                    shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"tree_shap_values\"] = shap_values #for SVC, this will contain the SVERAD values. Added if statement to deal with that. TODO: CHECK IF CORRECT!\n",
    "                shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"expected_value\"] = expected_value\n",
    "            shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"kernel_shap_values\"] = kernel_shap_values\n",
    "            shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"kernel_expected_value\"] = model_explainer.expected_value[1]\n",
    "            trial_df[\"kernel_present_shap\"] = (kernel_shap_values * dataset.feature_matrix.toarray()).sum(axis=1)\n",
    "            trial_df[\"kernel_absent_shap\"] = (kernel_shap_values * (1-dataset.feature_matrix.toarray())).sum(axis=1)\n",
    "            \n",
    "            prediction_df.append(trial_df)\n",
    "# prediction_df = pd.concat(prediction_df)\n",
    "# print(f\"Number of failed datasets: {failed}\")\n",
    "\n",
    "# print(\"Explanation done. Saving results...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'gamma_value': 0.01}\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'decision_function_shape': 'ovr', 'gamma_value': 0.01, 'max_iter': -1, 'no_player_value': 0, 'probability': True, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "-0.06944444444444445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': [0.1, 1, 10, 50, 100, 200, 400, 500, 750, 1000],\n",
       " 'gamma_value': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.best_params_)\n",
    "print(model.best_estimator_.get_params())\n",
    "print(model.best_score_)\n",
    "model_dict[\"parameter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0877a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy:  0.9024390243902439\n",
      "=============================\n",
      "Model accuracy:  0.9407665505226481\n",
      "=============================\n",
      "Model accuracy:  0.9337979094076655\n",
      "=============================\n",
      "Model accuracy:  0.8362369337979094\n",
      "=============================\n",
      "Model accuracy:  0.49825783972125437\n",
      "=============================\n",
      "Model accuracy:  0.49825783972125437\n",
      "=============================\n",
      "Model accuracy:  0.49825783972125437\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "# for g_val in model_dict[\"parameter\"][\"gamma_value\"]:\n",
    "#     mod = ExplainingSVC(gamma_value=g_val).fit(training_set.feature_matrix, training_set.label)\n",
    "#     preds = mod.predict(test_set.feature_matrix)\n",
    "#     print(\"Model accuracy: \", np.mean(preds == test_set.label))\n",
    "#     print(\"=============================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Storing all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59aadd5-b17c-4a50-bf03-e48a43a5e6dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./data/pickle/shap_dict_only_SVM.p\", \"wb\") as outfile:\n",
    "    pickle.dump(shap_dict, outfile)\n",
    "with open(\"./data/pickle/obtained_models_only_SVM.p\", \"wb\") as outfile:\n",
    "    pickle.dump(obtained_models, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction_df.to_csv(\"./data/prediction_df_SVM_RF.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
