{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Calculation of SVERAD SVs (for SVM) and SHAP values (for RF and SVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7f63d26",
   "metadata": {},
   "source": [
    "This notebook will generate:\n",
    "\n",
    "* Exact SVs for SVM with SVERAD\n",
    "* SHAP values for SVM with KernelSHAP\n",
    "* (Exact?) SVs for RF with TreeExplainer\n",
    "* SHAP values for RF with KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.sverad_svm import ExplainingSVC, create_SVC\n",
    "from src.sverad_kernel import rbf_kernel_matrix_sparse\n",
    "# from src.sverad_kernel import rbf_kernel_closure_function\n",
    "from src.utils import DataSet, UnfoldedMorganFingerprint, set_seeds\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "import dill #using dill since it allows more flexibility in pickling (nested functions)\n",
    "import shap\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm.auto import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57533678",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seeds(SEED)\n",
    "\n",
    "SAVE_DATASET_PICKLE = False\n",
    "LOAD_DATASET_PICKLE = True\n",
    "LOAD_PRECOMPUTED_EXPLANATIONS = False\n",
    "# SAVE_EXPLANATIONS = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Definition of models and searched hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f99c68d-da61-4f51-b115-9d46e6802def",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_list = [{\"name\": \"SVC\",\n",
    "               \"algorithm\": ExplainingSVC(),\n",
    "               \"parameter\": {\"C\": [0.1, 1, 10, 50, 100, 200, 400, 500, 750, 1000], \n",
    "                            \"gamma_value\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                            },\n",
    "              },\n",
    "              {\"name\": \"RF\",\n",
    "               \"algorithm\": ensemble.RandomForestClassifier(random_state=SEED, bootstrap=True, max_features=\"sqrt\"),\n",
    "               \"parameter\": {'n_estimators':[10, 100, 250, 500],\n",
    "                             'min_samples_split': [2, 3, 5, 7, 10,],\n",
    "                             'min_samples_leaf':[1, 2, 5, 10],\n",
    "                            },\n",
    "              },\n",
    "             ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the pre-compiled compound set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa0f5a4-922b-48cf-b206-b83c8d7a7160",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>active</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniprot_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P0DMS8</th>\n",
       "      <td>287</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label       active  random\n",
       "uniprot_id                \n",
       "P0DMS8         287     287"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(\"./data/dataset.tsv\", sep=\"\\t\")\n",
    "dataset_df.pivot_table(index=\"uniprot_id\", columns=\"label\", values=\"nonstereo_aromatic_smiles\", aggfunc=\"nunique\", fill_value=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c3f3f77-4538-49b1-8277-fada73a415da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dict = dict()\n",
    "fingerprint_gen_dict = dict()\n",
    "\n",
    "if not LOAD_DATASET_PICKLE:\n",
    "    for dataset_name, data_grpd_df in dataset_df.groupby(\"uniprot_id\"):\n",
    "        \n",
    "        # label: 1: active, 0: random\n",
    "        labels = np.array([1 if l == 'active' else 0 for l in data_grpd_df.label])\n",
    "        # Creating Fingerprint\n",
    "        morgan_radius2 = UnfoldedMorganFingerprint(radius=2)\n",
    "        morgan_radius2.fit_smiles(data_grpd_df.nonstereo_aromatic_smiles.tolist())\n",
    "        \n",
    "        # Constructing Dataset\n",
    "        fp_matrix = morgan_radius2.transform_smiles(data_grpd_df.nonstereo_aromatic_smiles.tolist())\n",
    "        # Constructing Dataset\n",
    "        dataset = DataSet(labels, fp_matrix)\n",
    "        dataset.add_attribute(\"nonstereo_aromatic_smiles\", data_grpd_df.nonstereo_aromatic_smiles.values)\n",
    "        \n",
    "        dataset_dict[dataset_name] = dataset\n",
    "        fingerprint_gen_dict[dataset_name] = morgan_radius2\n",
    "else:\n",
    "    with open(\"./data/pickle/dataset_dict.p\", \"rb\") as infile:\n",
    "        dataset_dict = dill.load(infile)\n",
    "    with open(\"./data/pickle/fingerprint_gen_dict.p\", \"rb\") as infile:\n",
    "        fingerprint_gen_dict = dill.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06da5da4-6871-4638-aba6-f03faabaa9d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_DATASET_PICKLE:\n",
    "    with open(\"./pickle_dumps/dataset_dict.pkl\", \"wb\") as outfile:\n",
    "        dill.dump(dataset_dict, outfile)\n",
    "    with open(\"./pickle_dumps/fingerprint_gen_dict.pkl\", \"wb\") as outfile:\n",
    "        dill.dump(fingerprint_gen_dict, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the models and generation of feature contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93538af2-4829-476f-b61e-c0fe0e10068f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_splits = 1 # Number of test-training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e071b433-9e2d-4348-93d9-c5142d25203b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd592e00b1b42fc9e4520ded47e7488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset.\n",
      "Model fitting and tuning to obtain optimal hyperparameters via Grid Search...\n",
      "Model accuracy:  0.9337979094076655\n",
      "Model trained and optimized.\n",
      "Model name:  SVC\n",
      "Explaining model using SVERAD...\n",
      "SVERAD values computation done.\n",
      "Explaining model using SHAP KernelExplainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d06978863c649d0b060071f82d44f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP KernelExplainer done.\n",
      "Model fitting and tuning to obtain optimal hyperparameters via Grid Search...\n",
      "Model accuracy:  0.9198606271777003\n",
      "Model trained and optimized.\n",
      "Model name:  RF\n",
      "Explaining model using SHAP TreeExplainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 1127/1148 [00:22<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP TreeExplainer done.\n",
      "Explaining model using SHAP KernelExplainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7988e0219e462cb5fef88e31f77e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP KernelExplainer done.\n",
      "Number of failed datasets: 0\n",
      "Explanation done. Saving results...\n"
     ]
    }
   ],
   "source": [
    "prediction_df = []\n",
    "failed = 0\n",
    "hyperparamter_dict = dict()\n",
    "obtained_models = dict()\n",
    "shap_dict = dict()\n",
    "\n",
    "# Loop over multiple data-sets. Here only one is assessed.\n",
    "for dataset_name, dataset in tqdm(dataset_dict.items(), total=len(dataset_dict)): #it takes ~5:30h to run Kernel SHAP for SVC + ~7h for RF\n",
    "\n",
    "    # Loop over test-training splits. Only one is assessed. `n_splits == 1`\n",
    "    print(\"Splitting dataset.\")\n",
    "    data_splitter = StratifiedShuffleSplit(n_splits=n_splits, random_state=SEED, test_size=0.50)\n",
    "    for trial_nr, (train_idx, test_idx) in tqdm(enumerate(data_splitter.split(dataset.feature_matrix, dataset.label)), leave=False, total=n_splits, disable=True):\n",
    "        training_set = dataset[train_idx]\n",
    "        test_set = dataset[test_idx]\n",
    "\n",
    "        #Iterating over assessed models.\n",
    "        for model_dict in model_list:\n",
    "            # print(model_dict[\"parameter\"])\n",
    "            # Setting up hyperparameter search.\n",
    "            param_grid = model_dict[\"parameter\"]\n",
    "            model = GridSearchCV(estimator = model_dict[\"algorithm\"],\n",
    "                                 param_grid = param_grid,\n",
    "                                 n_jobs=-1,\n",
    "                                 scoring= \"neg_mean_squared_error\", #\"neg_mean_squared_error\", \"accuracy\"\n",
    "                                 cv=StratifiedShuffleSplit(n_splits=10, random_state=SEED, test_size=0.5),\n",
    "                                verbose=0,\n",
    "                                )\n",
    "            # Determining optimal hyperparameters and fitting the model to the entire training set with these hyperparamters\n",
    "            print(\"Model fitting and tuning to obtain optimal hyperparameters via Grid Search...\")\n",
    "            model.fit(training_set.feature_matrix, training_set.label)\n",
    "            preds = model.predict(test_set.feature_matrix)\n",
    "            print(\"Model accuracy: \", np.mean(preds == test_set.label))\n",
    "            obtained_models[(dataset_name, trial_nr, model_dict[\"name\"])] = model\n",
    "            print(\"Model trained and optimized.\")\n",
    "            \n",
    "            # Saving hyperparameters\n",
    "            if model_dict[\"name\"] not in hyperparamter_dict:\n",
    "                hyperparamter_dict[model_dict[\"name\"]] = []\n",
    "            best_param = dict(model.best_params_)\n",
    "            best_param[\"dataset_name\"] = dataset_name\n",
    "            best_param[\"trial\"] = trial_nr\n",
    "            hyperparamter_dict[model_dict[\"name\"]].append(best_param)\n",
    "\n",
    "            # break\n",
    "            # SVs.\n",
    "            shap_values = None\n",
    "            print(\"Model name: \", model_dict[\"name\"])\n",
    "            if model_dict[\"name\"] == \"RF\":  # Random forest\n",
    "                pass \n",
    "                model_explainer = shap.TreeExplainer(model.best_estimator_, feature_perturbation=\"interventional\", data=training_set.feature_matrix.toarray())\n",
    "                try:\n",
    "                    # first line to sort out cases which fail on the spot\n",
    "                    _ = model_explainer.shap_values(test_set.feature_matrix.toarray()[0:2, :], check_additivity=True)  # check_additivity is True in defalult. This is just a reminder why I'm doing this.\n",
    "                    print(\"Explaining model using SHAP TreeExplainer...\")\n",
    "                    shap_values = model_explainer.shap_values(dataset.feature_matrix.toarray())[1]\n",
    "                    expected_value = model_explainer.expected_value[1]\n",
    "                    print(\"SHAP TreeExplainer done.\")\n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    failed += 1\n",
    "                    continue\n",
    "            elif model_dict[\"name\"] == \"SVC\":\n",
    "                print(\"Explaining model using SVERAD...\")\n",
    "                shap_values = model.best_estimator_.feature_weights(dataset.feature_matrix)\n",
    "                expected_value = model.best_estimator_.expected_value\n",
    "                print(\"SVERAD values computation done.\")\n",
    "            else:\n",
    "                raise(ValueError(\"Model not implemented.\"))\n",
    "            \n",
    "            # Kernel SHAP\n",
    "            shap_sample = shap.sample(training_set.feature_matrix)\n",
    "            if model_dict[\"name\"] == \"RF\":\n",
    "                link = \"identity\"\n",
    "            else:\n",
    "                link = \"logit\"\n",
    "            model_explainer = shap.KernelExplainer(model.predict_proba, shap_sample, link=link)\n",
    "            with warnings.catch_warnings():\n",
    "                # ignore all caught warnings. Necessary, since LassoLarsIC raises (a lot) future-warnings . See https://github.com/slundberg/shap/issues/2528\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                print(\"Explaining model using SHAP KernelExplainer...\")\n",
    "                kernel_shap_values = model_explainer.shap_values(dataset.feature_matrix, nsamples=\"auto\")[1]\n",
    "                print(\"SHAP KernelExplainer done.\")\n",
    "            # Creating a DataFrame with all relevant data.\n",
    "            trial_df = pd.DataFrame()\n",
    "            trial_df[\"nonstereo_aromatic_smiles\"] = dataset.nonstereo_aromatic_smiles\n",
    "            trial_df[\"dataset_idx\"] = range(len(dataset.label))\n",
    "            trial_df[\"label\"] = dataset.label\n",
    "            trial_df[\"prediction\"] = model.best_estimator_.predict(dataset.feature_matrix)\n",
    "            if model_dict[\"name\"] == \"SVC\":\n",
    "                trial_df[\"log_odds\"] = model.best_estimator_.predict_log_odds(dataset.feature_matrix)[:, 1]\n",
    "            trial_df[\"proba\"] = model.best_estimator_.predict_proba(dataset.feature_matrix)[:, 1]\n",
    "            trial_df[\"trainingset\"] = trial_df.nonstereo_aromatic_smiles.isin(training_set.nonstereo_aromatic_smiles)\n",
    "            trial_df[\"testset\"] = trial_df.nonstereo_aromatic_smiles.isin(test_set.nonstereo_aromatic_smiles)\n",
    "            trial_df[\"trial\"] = trial_nr\n",
    "            trial_df[\"dataset_name\"] = dataset_name\n",
    "            trial_df[\"algorithm\"] = model_dict[\"name\"]\n",
    "            \n",
    "            # Creating a data set storing all SVs and SHAP values.\n",
    "            shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])] = dict()\n",
    "            if shap_values is not None:\n",
    "                trial_df[\"present_shap\"] = (shap_values * dataset.feature_matrix.toarray()).sum(axis=1)\n",
    "                trial_df[\"absent_shap\"] = (shap_values * (1-dataset.feature_matrix.toarray())).sum(axis=1) \n",
    "                if model_dict[\"name\"] == \"SVC\":\n",
    "                    shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"sverad_values\"] = shap_values\n",
    "                else:\n",
    "                    shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"tree_shap_values\"] = shap_values #for SVC, this will contain the SVERAD values. Added if statement to deal with that. TODO: CHECK IF CORRECT!\n",
    "                shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"expected_value\"] = expected_value\n",
    "            shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"kernel_shap_values\"] = kernel_shap_values\n",
    "            shap_dict[(dataset_name, trial_nr, model_dict[\"name\"])][\"kernel_expected_value\"] = model_explainer.expected_value[1]\n",
    "            trial_df[\"kernel_present_shap\"] = (kernel_shap_values * dataset.feature_matrix.toarray()).sum(axis=1)\n",
    "            trial_df[\"kernel_absent_shap\"] = (kernel_shap_values * (1-dataset.feature_matrix.toarray())).sum(axis=1)\n",
    "            \n",
    "            prediction_df.append(trial_df)\n",
    "prediction_df = pd.concat(prediction_df)\n",
    "print(f\"Number of failed datasets: {failed}\")\n",
    "\n",
    "print(\"Explanation done. Saving results...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a69c9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "-0.09444444444444446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [10, 100, 250, 500],\n",
       " 'min_samples_split': [2, 3, 5, 7, 10],\n",
       " 'min_samples_leaf': [1, 2, 5, 10]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.best_params_)\n",
    "print(model.best_estimator_.get_params())\n",
    "print(model.best_score_)\n",
    "model_dict[\"parameter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0877a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g_val in model_dict[\"parameter\"][\"gamma_value\"]:\n",
    "#     mod = ExplainingSVC(gamma_value=g_val).fit(training_set.feature_matrix, training_set.label)\n",
    "#     preds = mod.predict(test_set.feature_matrix)\n",
    "#     print(\"Model accuracy: \", np.mean(preds == test_set.label))\n",
    "#     print(\"=============================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Storing all results.\n",
    "\n",
    "We use dill since it allows more flexibility. Needed for the nested function used for the custom SVC with RBF kernel. All the pickle dumps/load can be safely substituted with dill. Will do that in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b59aadd5-b17c-4a50-bf03-e48a43a5e6dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./data/pickle/shap_dict.p\", \"wb\") as outfile:\n",
    "    dill.dump(shap_dict, outfile)\n",
    "with open(\"./data/pickle/obtained_models.p\", \"wb\") as outfile:\n",
    "    # pickle.dump(obtained_models, outfile)\n",
    "    dill.dump(obtained_models, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved.\n"
     ]
    }
   ],
   "source": [
    "prediction_df.to_csv(\"./data/prediction_df.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
